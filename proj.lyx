#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass IEEEtran
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 1
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 1cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Hierarchical Hybrid Search Modification And Results 
\end_layout

\begin_layout Author
Igor Alexandrov - Rubi Mizrachi
\end_layout

\begin_layout Special Paper Notice
The Hebrew University of Jerusalem Jerusalem, Israel
\end_layout

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
In this article we wish to examine the performance of the Hierarchical search structure
 suggested by OgÌƒuzhan Erdem , Hoang Le and Viktor K.
 Prasanna 
\begin_inset CommandInset href
LatexCommand href
name "[Link]"
target "https://ieeexplore.ieee.org/document/6195565/"

\end_inset

 .
 They claimed that the regular Hierarchical search structure "Trie of Tries" has two main
 disadvantages: backtracking and inefficient memory usage due to variation
 in size of the trie nodes.
 They proposed a clustering algorithm that partitions the rule database into
 a fixed number of clusters that prevent backtracking.
 They have also proposed a variant of the trie tree, called 
\begin_inset Formula $T_{\epsilon}$
\end_inset

 , in which they fixed the problem of the inefficient memory usage by having
 1-to-1 relation between nodes in the tree and rules, creating a single
 node for each rule.
\begin_inset Newline newline
\end_inset

In our article we wish to validate the results achieved by the modified
 
\begin_inset Quotes eld
\end_inset

trie tree
\begin_inset Quotes erd
\end_inset

 , run tests over other classification algorithms and compare their performance.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In order to achieve our desired goal we implemented the Hierarchic
al search structure proposed in the article, as well as the regular Trie of Tries structure.
 First, we created a regular trie structure (a tree with one root and left child or right
 child if the next bit is zero or one, respectively and a list of fitting
 rules in each node).
 We also had to implement 
\begin_inset Formula $T_{\epsilon}$
\end_inset

 tree structure - which is kind of similar to the regular trie tree except
 of having 
\begin_inset Formula $\epsilon$
\end_inset

-branch property which means that a node can have a child node that doesn't
 depends on extra bit taken by the prefix IP address, and making
\begin_inset Formula $\epsilon-move$
\end_inset

 simply takes us to the next node - with the same prefix , but with a different
 rule , as said above, it prevents the situation of having multiple rules
 on the same node, causing inefficient memory usage. In order to get both
 of those structure working together - we also implemented a 
\begin_inset Quotes eld
\end_inset

TrieOfTries
\begin_inset Quotes erd
\end_inset

 structure which combined both of those structures into one, as presented
 in the article we seek to validate.
 
\end_layout

\begin_layout Section
Project Goals 
\end_layout

\begin_layout Standard
In this project we wish to answer the following questions: 
\end_layout

\begin_layout Itemize
Is it possible to implement the improved trie structure with a standard
 programming language (C++), without the use of proper hardware that is
 used by common routers (e.g SRAM) and still get adequate results?
\end_layout

\begin_layout Itemize
Are the results reliable? We wish to test both the original Trie of Tries
 and the improved Epsilon Tree Trie structure desribed in the article, and compare their performance.
\end_layout

\begin_layout Itemize
Does a modification in the rules database change the results? We wish
 to test the algorithm on a different databases of rules (uniform distribution
 databases and databases used by other researches).
\end_layout

\begin_layout Itemize
Is there any improvement we can suggest to be added to the "improved" algorithm?
 Or on the other hand, is there some feature in the "improved" algorithm that
 we think could be dismissed because it is unnecessary for achieving good
 results.
\end_layout

\begin_layout Itemize
How good are the results compared to new algorithms that are being used
 these days?
\end_layout

\begin_layout Section
Algorithms And Data Structure Explained
\end_layout

\begin_layout Subsection
Clustering Algorithm
\end_layout

\begin_layout Standard
The tree structure we wish to achieve will have 2 stages: SA (source address)
 stage and DA (destination stage) on which it will be derived from the rules
 source address and destination address respectively , this 2-staged structure
 will eliminate backtracking from SA to DA.
 The algorithm takes a set of prefixes S and the number of clusters R as
 its, and genrates a collection of non-empty subsets {
\begin_inset Formula $S_{i}\}$
\end_inset

, (0
\begin_inset Formula $\leq i$
\end_inset

<R).
 Each subset is called a cluster.
 in each cluster the SA prefixes are pairwise disjoint.
\begin_inset Newline newline
\end_inset

initially, the algorithm builds a uni-bit trie using the SA prefixes.
 afterward recursive leaf extraction is performed.
 Each recursive Step i of the algorithm includes two sub-steps:(1) the leaf
 nodes are removed from the trie and moved into set {
\begin_inset Formula $S_{i}\}$
\end_inset

 and (2) the non-prefix leaf nodes are trimmed off the trie.
 in fig 1 below the algorithm is presented and a simple example of the first
 iteration of it.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename switch1.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Standard
fig 1
\end_layout

\begin_layout Subsection
Stage One Of The Structure (SA tree)
\end_layout

\begin_layout Standard
The stage one of the tree is simply a binary search tree (BST) and it is
 built separately for each one of the clusters.
 Each node in the SA tree includes:(1) a value (prefix of source address)
 (2) a prefix length, (3) left pointer and (4)right pointer.The left sub-tree
 of a node contains only values that are less than or equal to the value
 stored in that node.Before constructing the BST, prefixes are sorted into
 an array in ascending order.
 Given the sorted array of prefixes, the BST is constructed by picking the
 correct prefix and recursively building the left and the right subtrees.
 As convention , all prefixes are extended to the length of the longest
 prefix by appending with '0' bits.
 
\begin_inset Newline newline
\end_inset

As mentioned before, using the cluster algorithm causes the prefixes in any
 cluster to be pairwise disjoint,hence , extending the prefixes to the same
 length should not cause overlapping.
\end_layout

\begin_layout Subsection
Stage Two Of The Structure (DA tree)
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $T_{\epsilon}$
\end_inset

 Tree Structure 
\end_layout

\begin_layout Standard
In the previous paragraph we created the first stage of the structure, now each
 node we created in previous stage is connected to a DA trie.
 Therefore, in each cluster, the number of DA tries equal the number
 of SA prefixes.
 Each prefix node of a DA trie stores at least one rule.
 In this structure each overlapped rule is stored in nodes rather than pointing
 to a list of these rules, this structure is called, as mentioned above 
\begin_inset Formula $T_{\epsilon}$
\end_inset

 tree, a single node in 
\begin_inset Formula $T_{\epsilon}$
\end_inset

 may have a single epsilon branch for which no input bit is consumed or
 '0' and
\backslash
or '1' branch, but can't have both epsilon branch with 0 or 1 branch.
\end_layout

\begin_layout Subsubsection
Path Compression (PC) 
\end_layout

\begin_layout Standard
\begin_inset Formula $T_{\epsilon}$
\end_inset

 structure can also be compressed , each non-prefix node of a trie can be
 removed if it has a prefix node from the root.
 in order to keep record of the nodes that were deleted , each node must
 store a skip value (SV) - representing the number of bits that were skipped
 and bit string (BS) - representing the exact bits that were skipped.In our
 tests we deduce that the Path Compression significantly decreases the number
 of nodes traversed - and the reason is quite straight
 forward - instead of traversing nodes in the trie with only one
 child that don't contain any rule within , you just skip them using BV,SV
 - but on the other hand it appears that there is a space - time tradeoff
 , because BV,SV require every node to preserve some amount of bits in order
 to store those two fields.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename switch4.png

\end_inset


\end_layout

\begin_layout Standard
fig 2 - example of two staged tree of tries with path compression.
\end_layout

\begin_layout Subsection
Multiprocessing 
\end_layout

\begin_layout Standard
In order to achieve better results under this algorithm, one must use parallel processing
over the clusters - which were created the way they created only for
 that reason - in each cluster there are no two rules that one is a prefix
 of the other one, in other words - every two rules in a cluster are not
 a proper prefix of one another.
 In order to make sure there are no backtracking within each cluster, and
 every cluster can be processed separately and independently by the processors.
 This method of working appears to be extremely efficient when dealing with
 high number of rules, which causes a regular trie tree be extremely complex
 and the traversal over it is very inefficient.
 A clusters structure like the one that was presented in the article turns
 out to be very efficient for these cases.
\begin_inset Newline newline
\end_inset

We used Ptheard C library in order to execute the search for a matching
 rule in parallel, each thread got a different cluster, the same header , and
 a shared memory that contained the best rule found by one of the threads
 until now.
 A mutex lock was employed to make sure that the shared variable is used
 properly.
 
\end_layout

\begin_layout Section
Main advantages 
\end_layout

\begin_layout Standard
As the writers of the article mention, the advantages they seek to
 attain when compared to a regular Trie of Tries are skipping the backtracking part,
 which 
\begin_inset Quotes eld
\end_inset

costs
\begin_inset Quotes erd
\end_inset

 a lot and makes it a lot less comfortable to work with multiprocessing
 method.
 the clustering algorithm indeed prevents the situation of backtracking
 , even though it demands us to examine multiple objects (clusters) - we
 don't mind - because all of this is intended to be done in parallel.
 One more other thing that is prevented is the inefficiency with node
 space, because of the 
\begin_inset Quotes eld
\end_inset

unlimited
\begin_inset Quotes erd
\end_inset

 size of the rule list in each node, the nodes could get bigger and bigger and a limit is
 needed to be set, therefore, the authors have simplified this matter by
 setting a limit to the number of rules that might be inserted into a single
 node, otherwise an 
\begin_inset Quotes eld
\end_inset

epsilon-node
\begin_inset Quotes erd
\end_inset

 will be created and the rest of the rules will be stored in it.
 
\end_layout

\begin_layout Section
Main disadvantages
\end_layout

\begin_layout Standard
During our research and while implementing the algorithm we tried to inspect
 and give an estimation of the effectiveness of the additional features of the Epsilon Tree Trie 
 and came to some conclusions
 in the matter.
\begin_inset Newline newline
\end_inset

When implelemnting the Path Compress process, we noticed that it is relatively expensive
 and therefore can only feasibly be done once, after the initial rule database is isnerted into the structure
 of the tree (rather than on-the-fly , while adding rule by rule) therefore
 it causes some drawbacks - first it is very time expensive , as will be
 shown below in the results section, the running time of tree construction
 without PC is much bigger than the same tree without PC feature.
 We could neglect that fact if the rule-search running time of a compressed
 tree were much faster than a regular tree, but as will be presented below
 as well, this is not the case - the way we see it - path compress doesn't
 save a lot of time while searching through the tree, and the number of
 
\begin_inset Quotes eld
\end_inset

visited nodes
\begin_inset Quotes erd
\end_inset

 is quite the same with or without PC.
 One other major disadvantage that we encountered is the fact that modifying
 a tree structure, for example, adding rule after compressing is becoming
 more sophisticated and it's a shame that it's not that easy compared to
 the regular structure, in which adding and removing nodes or rules can be on
 the fly.
\end_layout

\begin_layout Section
Experiments & Results
\end_layout

\begin_layout Standard
we have tested the improved tree trie after fully implementing it over couple
 of the databases.
 We wanted to make sure it performs well over all kinds of data sets, therefore
 we used a generator in order to create rules and headers to be inspected,
 both from a random uniform distribution and the test cases provided by CATE (Classification
 Algorithms Testing Environment)
\begin_inset CommandInset href
LatexCommand href
name " [LINK] "
target "https://github.com/gusew/cate"

\end_inset

 ,the reason we used CATE database is that we wanted to inspect the improved
 trie over a realistic database and not just on random data , that may or
 may not have any connection to real life network traffic.
\end_layout

\begin_layout Subsection
Results Over Randomly Generated Data 
\end_layout

\begin_layout Subsubsection
Differences 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename diff.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
as presented above those are the differences between the number of rules
 ***
\end_layout

\begin_layout Subsubsection
Memory Usage
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename memory usage.png

\end_inset


\end_layout

\begin_layout Standard
The estimated memory usage of the regular trie (produced by examining the size of the largest
 nodes in both levels of the structure and multiplying those sizes by the total number of nodes)
 grows linearly with the number of rules. On the
 other hand, the estimated memory usage of the new structure is almost constant by comparison
 , which makes a lot of sense because the regular nodes will just keep adding
 rules to their lists (causing the required memory for all nodes to increase dramatically) while
 in the improved method there is a one-to-one relationship between nodes
 and rules, therefore adding a bunch of rules will not significantly alter
 the size of a specific node, just add a bunch of nodes of the same size, 
 which wouldn't increase the memory demands nearly as much.
\end_layout

\begin_layout Subsubsection
Bytes per rule
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename bytes per rule.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
bytes per rule***
\end_layout

\begin_layout Subsubsection
Running Time 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename runtime.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
As you can see, the Tree Trie Epsilon structure seems to perform better on uniformly
 random rules, and as the rule database gets larger so does the difference in runtime.
 Judging by these results, it seems that the authors of
 the article managed to achieve a real improvement theat only gets better with more rules.
\end_layout

\begin_layout Subsection
Results Over Realistic Data
\end_layout

\begin_layout Standard
In this section we took some pre-made data from CATE and run our tests over
 it and got surprising results that raise questions about the proposed structure
, and it's performance in "real" conditions.
\end_layout

\begin_layout Standard
In terms of memory it seems like the upgraded algorithm still performs better
 than the regular Trie, because of the same reasons we mentioned above. Nevertheless
 , as we will show below, in terms of running time it seems that on realistic
 data the old-fashioned Trie of Tries performs better than the new structure.
\end_layout

\begin_layout Subsubsection
Differences
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename diff-cate.png

\end_inset


\end_layout

\begin_layout Standard
differences over CATE's data base ***
\end_layout

\begin_layout Subsubsection
Memory Usage
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename memory usage- cate.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
memory usage turns to be much similar over CATE's database to a fully randomized
 data.
\end_layout

\begin_layout Subsubsection
Bytes per rule
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename bytes per rule-cate.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
here there is also a significant increase in memory consumption per rule
 in the Trie of Tries compared to the Tree Trie Epsilon.
\end_layout

\begin_layout Subsubsection
Running Time 
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename runtime-cate.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
Here on the data taken from CATE database it seems like the regular trie
 keeps almost constant , low running time, but on the other hand our improved
 algorithm is getting much worse when raising the number of rules, and didn't
 manage to be faster than the original one at any point of the test. This is very surprising,
 and it isn't intuitively obvious why this would be the case. 
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
our goals were to deeply examine the introduced algorithm , so first we
 implemented it carefully, using all the features that were introduced by
 the authors, then we created our own rule database based on randomly generated (which
 may or may not reflect on real situations in the real world) we run tests and
 got some promising results, on the other hand, when testing on more reliable
 data we got some disturbing results that undermined the efficiency of the
 Tree Trie Epsilon - the algorithm that the were developed in the article
 we choose to examine, we made sure that our data structures were made exactly
 as the authors suggested in their article, so we are finding it hard to
 believe that the results were faulty , but we keep in mind the maybe some
 fault led to this results, and further research needs to be made -maybe
 on some real traffic from other sources, maybe on an implemented algorithm
 used on an actual router - because we still performed those tests on a
 virtual environment (PC) and not on a real network hardware devices - maybe
 some inaccuracies were made that couldn't be prevented using a computer.
 
\end_layout

\begin_layout Standard
Although the results were inconclusive we find this new structure to be very promising. 
The surprising runtime results when using the CATE test case rules are an obvious direction for further analysis, but the 
unambigous and increasing advantage in memory usage alone is significant enough to make the Epsilon Trtee Trie structure worty
of greater attention.  
\end_layout

\end_body
\end_document
